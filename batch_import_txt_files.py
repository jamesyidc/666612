#!/usr/bin/env python3
"""
æ‰¹é‡å¯¼å…¥2026-01-03çš„æ‰€æœ‰TXTæ–‡ä»¶åˆ°æ•°æ®åº“
"""
import requests
from bs4 import BeautifulSoup
import re
import sqlite3
from datetime import datetime
import pytz
import time
import sys

# å¯¼å…¥gdrive_final_detectorä¸­çš„è§£æå‡½æ•°
sys.path.insert(0, '/home/user/webapp')
from gdrive_final_detector import parse_content

# é…ç½®
TODAY_FOLDER_ID = "1euzRzLjPDl08ZTvdDM_H_6hJzVNFldfQ"  # 2026-01-03æ–‡ä»¶å¤¹
DB_PATH = "/home/user/webapp/databases/crypto_data.db"
BEIJING_TZ = pytz.timezone('Asia/Shanghai')

def log(msg):
    """æ‰“å°æ—¥å¿—"""
    print(f"[{datetime.now(BEIJING_TZ).strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def get_all_txt_files():
    """è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰TXTæ–‡ä»¶"""
    log(f"ğŸ“‚ è®¿é—®æ–‡ä»¶å¤¹: {TODAY_FOLDER_ID}")
    url = f"https://drive.google.com/embeddedfolderview?id={TODAY_FOLDER_ID}"
    
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        all_links = soup.find_all('a', href=True)
        txt_files = []
        
        for link in all_links:
            href = link.get('href', '')
            filename = link.get_text(strip=True)
            
            if filename.endswith('.txt'):
                # æå–æ–‡ä»¶ID
                if '/file/d/' in href:
                    match = re.search(r'/file/d/([a-zA-Z0-9_-]+)', href)
                    if match:
                        file_id = match.group(1)
                        txt_files.append({
                            'name': filename,
                            'id': file_id
                        })
        
        # æŒ‰æ–‡ä»¶åæ’åº
        txt_files.sort(key=lambda x: x['name'])
        log(f"âœ… æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶")
        return txt_files
        
    except Exception as e:
        log(f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return []



def download_and_parse_file(file_info):
    """ä¸‹è½½å¹¶è§£æå•ä¸ªæ–‡ä»¶"""
    file_id = file_info['id']
    filename = file_info['name']
    
    try:
        # ä¸‹è½½æ–‡ä»¶å†…å®¹
        url = f"https://drive.google.com/uc?export=download&id={file_id}"
        response = requests.get(url, timeout=10)
        response.encoding = 'utf-8'
        content = response.text
        
        # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
        # æ–‡ä»¶åæ ¼å¼: 2026-01-03_1952.txt -> 2026-01-03 19:52:00
        filename_match = re.search(r'(\d{4}-\d{2}-\d{2})_(\d{2})(\d{2})\.txt', filename)
        if not filename_match:
            return None
        
        date_str = filename_match.group(1)
        hour = filename_match.group(2)
        minute = filename_match.group(3)
        file_timestamp = f"{date_str} {hour}:{minute}:00"
        
        # ä½¿ç”¨gdrive_final_detectorçš„è§£æå‡½æ•°
        data = parse_content(content, file_timestamp)
        
        if data:
            return data
        else:
            return None
            
    except Exception as e:
        log(f"   âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None

def import_to_database(data):
    """å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
    try:
        conn = sqlite3.connect(DB_PATH, timeout=60)
        conn.execute("PRAGMA journal_mode=WAL")
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        cursor.execute("""
            SELECT COUNT(*) FROM crypto_snapshots 
            WHERE snapshot_time = ?
        """, (data['snapshot_time'],))
        
        if cursor.fetchone()[0] > 0:
            conn.close()
            return False  # å·²å­˜åœ¨
        
        # æ’å…¥æ•°æ®
        cursor.execute("""
            INSERT INTO crypto_snapshots 
            (snapshot_time, snapshot_date, rush_up, rush_down, diff, count, status, count_score_display, count_score_type, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, datetime('now', '+8 hours'))
        """, (
            data['snapshot_time'],
            data['snapshot_date'],
            data['rush_up'],
            data['rush_down'],
            data['diff'],
            data['count'],
            data['status'],
            data['count_score_display'],
            data['count_score_type']
        ))
        
        conn.commit()
        conn.close()
        return True  # å¯¼å…¥æˆåŠŸ
        
    except Exception as e:
        log(f"   âŒ æ•°æ®åº“é”™è¯¯: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    log("=" * 80)
    log("ğŸš€ å¼€å§‹æ‰¹é‡å¯¼å…¥2026-01-03çš„æ‰€æœ‰TXTæ–‡ä»¶")
    log("=" * 80)
    
    # 1. è·å–æ‰€æœ‰æ–‡ä»¶
    txt_files = get_all_txt_files()
    
    if not txt_files:
        log("âŒ æœªæ‰¾åˆ°ä»»ä½•TXTæ–‡ä»¶")
        return
    
    total = len(txt_files)
    success_count = 0
    skip_count = 0
    error_count = 0
    
    log(f"\nğŸ“Š å‡†å¤‡å¯¼å…¥ {total} ä¸ªæ–‡ä»¶...\n")
    
    # 2. é€ä¸ªå¤„ç†æ–‡ä»¶
    for i, file_info in enumerate(txt_files, 1):
        filename = file_info['name']
        log(f"[{i}/{total}] å¤„ç†: {filename}")
        
        # ä¸‹è½½å¹¶è§£æ
        data = download_and_parse_file(file_info)
        
        if data:
            # å¯¼å…¥æ•°æ®åº“
            result = import_to_database(data)
            
            if result:
                success_count += 1
                log(f"   âœ… å¯¼å…¥æˆåŠŸ: {data['snapshot_time']} | æ€¥æ¶¨{data['rush_up']} æ€¥è·Œ{data['rush_down']} | {data['status']}")
            else:
                skip_count += 1
                log(f"   â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡")
        else:
            error_count += 1
            log(f"   âŒ è§£æå¤±è´¥")
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.5)
    
    # 3. ç»Ÿè®¡ç»“æœ
    log("\n" + "=" * 80)
    log("ğŸ“Š æ‰¹é‡å¯¼å…¥å®Œæˆï¼")
    log("=" * 80)
    log(f"   æ€»æ–‡ä»¶æ•°: {total}")
    log(f"   âœ… æˆåŠŸå¯¼å…¥: {success_count}")
    log(f"   â­ï¸  è·³è¿‡(å·²å­˜åœ¨): {skip_count}")
    log(f"   âŒ å¤±è´¥: {error_count}")
    log("=" * 80)

if __name__ == '__main__':
    main()
