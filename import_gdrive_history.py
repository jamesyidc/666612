#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»Google Driveæ‰¹é‡å¯¼å…¥å†å²æ•°æ®åˆ°æ•°æ®åº“
"""

import asyncio
import sys
import os
from datetime import datetime
from playwright.async_api import async_playwright
from crypto_database import CryptoDatabase

# Google Driveæ–‡ä»¶å¤¹é…ç½®
ROOT_FOLDER_ID = '1j8YV6KysUCmgcmASFOxztWWIE1Vq-kYV'
ROOT_FOLDER_URL = f'https://drive.google.com/drive/folders/{ROOT_FOLDER_ID}'

def parse_home_data(content):
    """è§£æé¦–é¡µæ•°æ®å†…å®¹"""
    lines = content.strip().split('\n')
    
    stats = {}
    coins = []
    
    in_coin_section = False
    
    for line in lines:
        line = line.strip()
        
        # è§£æç»Ÿè®¡æ•°æ®
        if line.startswith('é€æ˜æ ‡ç­¾_'):
            parts = line.split('=')
            if len(parts) == 2:
                key = parts[0].replace('é€æ˜æ ‡ç­¾_', '')
                value = parts[1]
                
                if 'æ€¥æ¶¨æ€»å’Œ' in key:
                    stats['rushUp'] = value.split('ï¼š')[1] if 'ï¼š' in value else value
                elif 'æ€¥è·Œæ€»å’Œ' in key:
                    stats['rushDown'] = value.split('ï¼š')[1] if 'ï¼š' in value else value
                elif 'äº”ç§çŠ¶æ€' in key:
                    stats['status'] = value.split('ï¼š')[1] if 'ï¼š' in value else value
                elif 'æ€¥æ¶¨æ€¥è·Œæ¯”å€¼' in key:
                    stats['ratio'] = value.split('ï¼š')[1] if 'ï¼š' in value else value
                elif 'ç»¿è‰²æ•°é‡' in key:
                    stats['greenCount'] = value
                elif 'ç™¾åˆ†æ¯”' in key:
                    stats['percentage'] = value
                elif 'è®¡æ¬¡' in key:
                    stats['count'] = value
                elif 'å·®å€¼ç»“æœ' in key:
                    stats['diff'] = value.split('ï¼š')[1] if 'ï¼š' in value else value
        
        # å¸ç§æ•°æ®
        if '[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µå¼€å§‹]' in line:
            in_coin_section = True
            continue
        
        if '[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µç»“æŸ]' in line:
            break
        
        if in_coin_section and '|' in line:
            parts = line.split('|')
            if len(parts) >= 16:
                coin = {
                    'index': parts[0],
                    'symbol': parts[1],
                    'change': parts[2],
                    'rushUp': parts[3],
                    'rushDown': parts[4],
                    'updateTime': parts[5],
                    'highPrice': parts[6],
                    'highTime': parts[7],
                    'decline': parts[8],
                    'change24h': parts[9],
                    'rank': parts[12],
                    'currentPrice': parts[13],
                    'ratio1': parts[14],
                    'ratio2': parts[15]
                }
                coins.append(coin)
    
    # è·å–æ›´æ–°æ—¶é—´
    update_time = coins[0]['updateTime'] if coins else ''
    
    return {
        'stats': stats,
        'coins': coins,
        'updateTime': update_time
    }

async def get_all_date_folders(page):
    """è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹"""
    print(f"\n{'='*60}")
    print(f"1. è®¿é—®æ ¹æ–‡ä»¶å¤¹...")
    print(f"{'='*60}")
    
    await page.goto(ROOT_FOLDER_URL, wait_until='networkidle', timeout=60000)
    await page.wait_for_timeout(3000)
    
    # è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹ï¼ˆæ ¼å¼: 2025-12-03ï¼‰
    date_folders = []
    
    # è·å–é¡µé¢æ–‡æœ¬å†…å®¹
    content = await page.content()
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…æ—¥æœŸæ ¼å¼çš„æ–‡ä»¶å¤¹å
    import re
    pattern = r'2025-\d{2}-\d{2}'
    matches = re.findall(pattern, content)
    
    # å»é‡
    date_folders = list(set(matches))
    date_folders.sort(reverse=True)  # é™åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    
    print(f"\næ‰¾åˆ° {len(date_folders)} ä¸ªæ—¥æœŸæ–‡ä»¶å¤¹:")
    for folder in date_folders[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"   - {folder}")
    if len(date_folders) > 10:
        print(f"   ... è¿˜æœ‰ {len(date_folders) - 10} ä¸ªæ–‡ä»¶å¤¹")
    
    return date_folders

async def get_files_in_date_folder(page, date_str):
    """è·å–æŒ‡å®šæ—¥æœŸæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    print(f"\n{'='*60}")
    print(f"å¤„ç†æ—¥æœŸ: {date_str}")
    print(f"{'='*60}")
    
    # å›åˆ°æ ¹æ–‡ä»¶å¤¹
    await page.goto(ROOT_FOLDER_URL, wait_until='networkidle', timeout=60000)
    await page.wait_for_timeout(2000)
    
    # æŸ¥æ‰¾å¹¶ç‚¹å‡»æ—¥æœŸæ–‡ä»¶å¤¹
    try:
        folder_locator = page.locator(f'text="{date_str}"').first
        await folder_locator.dblclick(timeout=10000)
        await page.wait_for_timeout(3000)
        print(f"âœ… å·²è¿›å…¥ {date_str} æ–‡ä»¶å¤¹")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿›å…¥æ–‡ä»¶å¤¹ {date_str}: {e}")
        return []
    
    # ç‚¹å‡»æ’åºæŒ‰é’®
    try:
        sort_button = page.locator('button[aria-label*="Sort"], button[aria-label*="æ’åº"]').first
        await sort_button.click(timeout=5000)
        await page.wait_for_timeout(1000)
        
        # ç‚¹å‡»"ä¿®æ”¹æ—¶é—´"é€‰é¡¹
        modified_option = page.locator('text="Modified", text="ä¿®æ”¹æ—¶é—´"').first
        await modified_option.click(force=True, timeout=5000)
        await page.wait_for_timeout(2000)
        print("âœ… å·²æŒ‰ä¿®æ”¹æ—¶é—´æ’åº")
    except Exception as e:
        print(f"âš ï¸ æ’åºå¤±è´¥ï¼ˆå°†ä½¿ç”¨é»˜è®¤é¡ºåºï¼‰: {e}")
    
    # è·å–é¡µé¢å†…å®¹å¹¶æå–æ–‡ä»¶å
    content = await page.content()
    
    import re
    pattern = rf'{date_str}_\d{{4}}\.txt'
    matches = re.findall(pattern, content)
    
    # å»é‡å¹¶æ’åº
    files = list(set(matches))
    files.sort()  # æŒ‰æ—¶é—´æ’åº
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªTXTæ–‡ä»¶")
    
    return files

async def read_file_content(page, filename):
    """è¯»å–æŒ‡å®šæ–‡ä»¶çš„å†…å®¹"""
    try:
        # æŸ¥æ‰¾å¹¶ç‚¹å‡»æ–‡ä»¶
        file_locator = page.locator(f'text="{filename}"').first
        await file_locator.click(timeout=10000)
        await page.wait_for_timeout(3000)
        
        # å°è¯•ä»iframeä¸­è¯»å–å†…å®¹
        frames = page.frames
        
        for i, frame in enumerate(frames):
            try:
                frame_url = frame.url
                if 'docs.google.com' in frame_url or 'drive.google.com' in frame_url:
                    text_content = await frame.content()
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®æ•°æ®
                    if '[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µå¼€å§‹]' in text_content or 'BTC' in text_content:
                        # æå–çº¯æ–‡æœ¬
                        import re
                        # ç§»é™¤HTMLæ ‡ç­¾
                        clean_text = re.sub(r'<[^>]+>', '\n', text_content)
                        clean_text = re.sub(r'\n+', '\n', clean_text)
                        
                        # æŸ¥æ‰¾æ•°æ®åŒºåŸŸ
                        if '[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µå¼€å§‹]' in clean_text:
                            start_idx = clean_text.find('[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µå¼€å§‹]')
                            end_idx = clean_text.find('[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µç»“æŸ]', start_idx)
                            
                            if end_idx > start_idx:
                                data_section = clean_text[start_idx:end_idx + len('[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µç»“æŸ]')]
                                
                                # ä¹ŸåŒ…å«ç»Ÿè®¡æ•°æ®ï¼ˆåœ¨æ•°æ®åŒºåŸŸä¹‹å‰ï¼‰
                                stats_start = max(0, start_idx - 2000)
                                full_content = clean_text[stats_start:end_idx + len('[è¶…çº§åˆ—è¡¨æ¡†_é¦–é¡µç»“æŸ]')]
                                
                                return full_content
            except Exception as frame_error:
                continue
        
        return None
        
    except Exception as e:
        print(f"   âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None
    finally:
        # å…³é—­é¢„è§ˆï¼ˆæŒ‰ESCé”®ï¼‰
        await page.keyboard.press('Escape')
        await page.wait_for_timeout(1000)

async def import_all_history():
    """å¯¼å…¥æ‰€æœ‰å†å²æ•°æ®"""
    db = CryptoDatabase()
    
    print("="*60)
    print("å¼€å§‹å¯¼å…¥Google Driveå†å²æ•°æ®")
    print("="*60)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            # 1. è·å–æ‰€æœ‰æ—¥æœŸæ–‡ä»¶å¤¹
            date_folders = await get_all_date_folders(page)
            
            if not date_folders:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¥æœŸæ–‡ä»¶å¤¹")
                return
            
            total_imported = 0
            total_skipped = 0
            
            # 2. é€ä¸ªå¤„ç†æ—¥æœŸæ–‡ä»¶å¤¹
            for date_str in date_folders:
                print(f"\n{'='*60}")
                print(f"æ­£åœ¨å¤„ç†: {date_str}")
                print(f"{'='*60}")
                
                # è·å–è¯¥æ—¥æœŸä¸‹çš„æ‰€æœ‰æ–‡ä»¶
                files = await get_files_in_date_folder(page, date_str)
                
                if not files:
                    print(f"âš ï¸ {date_str} æ–‡ä»¶å¤¹ä¸ºç©º")
                    continue
                
                # 3. é€ä¸ªè¯»å–æ–‡ä»¶
                for idx, filename in enumerate(files, 1):
                    # è§£ææ–‡ä»¶åè·å–æ—¶é—´
                    # æ ¼å¼: 2025-12-03_1012.txt -> 2025-12-03 10:12:00
                    parts = filename.replace('.txt', '').split('_')
                    if len(parts) != 2:
                        continue
                    
                    date_part = parts[0]
                    time_part = parts[1]
                    
                    # æ ¼å¼åŒ–æ—¶é—´
                    hour = time_part[:2]
                    minute = time_part[2:]
                    snapshot_time = f"{date_part} {hour}:{minute}:00"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = db.get_snapshot_by_time(snapshot_time)
                    if existing:
                        print(f"   [{idx}/{len(files)}] â­ï¸  {filename} (å·²å­˜åœ¨ï¼Œè·³è¿‡)")
                        total_skipped += 1
                        continue
                    
                    print(f"   [{idx}/{len(files)}] ğŸ“¥ æ­£åœ¨å¯¼å…¥: {filename}")
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    content = await read_file_content(page, filename)
                    
                    if not content:
                        print(f"      âŒ æ— æ³•è¯»å–å†…å®¹")
                        continue
                    
                    # è§£ææ•°æ®
                    try:
                        parsed_data = parse_home_data(content)
                        
                        if not parsed_data['coins']:
                            print(f"      âŒ è§£æå¤±è´¥ï¼ˆæ— å¸ç§æ•°æ®ï¼‰")
                            continue
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        snapshot_id = db.save_snapshot(
                            data=parsed_data['coins'],
                            stats=parsed_data['stats'],
                            snapshot_time=snapshot_time,
                            filename=filename
                        )
                        
                        print(f"      âœ… å·²ä¿å­˜ (ID: {snapshot_id}, {len(parsed_data['coins'])}ä¸ªå¸ç§)")
                        total_imported += 1
                        
                    except Exception as parse_error:
                        print(f"      âŒ è§£æ/ä¿å­˜å¤±è´¥: {parse_error}")
                        continue
                    
                    # æ¯5ä¸ªæ–‡ä»¶ä¼‘æ¯ä¸€ä¸‹
                    if idx % 5 == 0:
                        await page.wait_for_timeout(2000)
            
            print(f"\n{'='*60}")
            print(f"å¯¼å…¥å®Œæˆ!")
            print(f"{'='*60}")
            print(f"âœ… æˆåŠŸå¯¼å…¥: {total_imported} ä¸ªå¿«ç…§")
            print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨: {total_skipped} ä¸ªå¿«ç…§")
            print(f"ğŸ“Š æ€»è®¡å¤„ç†: {total_imported + total_skipped} ä¸ªå¿«ç…§")
            
            # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
            stats = db.get_statistics()
            print(f"\næ•°æ®åº“ç»Ÿè®¡:")
            print(f"   å¿«ç…§æ€»æ•°: {stats['total_snapshots']}")
            print(f"   æœ€æ—©æ—¶é—´: {stats['earliest_time']}")
            print(f"   æœ€æ™šæ—¶é—´: {stats['latest_time']}")
            
        finally:
            await browser.close()

if __name__ == '__main__':
    asyncio.run(import_all_history())
