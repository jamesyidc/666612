#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ†ä»“æ•°æ®çˆ¬è™«
ä» https://history.btc123.fans/baocang/ è·å–å®æ—¶çˆ†ä»“æ•°æ®
"""

import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import time

def scrape_liquidation_data():
    """
    çˆ¬å–çˆ†ä»“æ•°æ®
    è¿”å›æ ¼å¼:
    {
        'hour_1_amount': float,  # 1å°æ—¶çˆ†ä»“é‡‘é¢ï¼ˆç¾å…ƒï¼‰
        'hour_24_amount': float,  # 24å°æ—¶çˆ†ä»“é‡‘é¢ï¼ˆç¾å…ƒï¼‰
        'hour_24_people': int,   # 24å°æ—¶çˆ†ä»“äººæ•°
        'total_position': float,  # å…¨ç½‘æŒä»“é‡ï¼ˆç¾å…ƒï¼‰
        'panic_index': float,     # ææ…Œæ¸…æ´—æŒ‡æ•° = 24å°æ—¶çˆ†ä»“äººæ•°/å…¨ç½‘æŒä»“é‡
        'update_time': str,       # æ›´æ–°æ—¶é—´
        'raw_data': dict         # åŸå§‹æ•°æ®
    }
    """
    url = "https://history.btc123.fans/baocang/"
    
    try:
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        html = response.text
        
        # ä»HTMLä¸­æå–Vueæ•°æ®
        # æŸ¥æ‰¾APIç«¯ç‚¹æˆ–å†…åµŒçš„JSONæ•°æ®
        soup = BeautifulSoup(html, 'html.parser')
        
        # æ–¹æ³•1: æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„æ•°æ®
        scripts = soup.find_all('script')
        data_found = False
        result = {}
        
        for script in scripts:
            script_text = script.string
            if script_text and 'blast' in script_text.lower():
                # å°è¯•æå–æ•°æ®
                try:
                    # æŸ¥æ‰¾ç±»ä¼¼ data: {...} çš„æ¨¡å¼
                    match = re.search(r'data[:\s]*{([^}]+)}', script_text)
                    if match:
                        print(f"æ‰¾åˆ°æ•°æ®æ®µ: {match.group(0)[:100]}...")
                except:
                    pass
        
        # æ–¹æ³•2: ç›´æ¥è¯·æ±‚APIç«¯ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # é€šå¸¸è¿™ç±»ç½‘ç«™ä¼šæœ‰ /api/baocang æˆ–ç±»ä¼¼çš„ç«¯ç‚¹
        api_urls = [
            'https://history.btc123.fans/api/baocang',
            'https://history.btc123.fans/baocang/api',
            'https://api.btc123.fans/baocang',
        ]
        
        for api_url in api_urls:
            try:
                api_response = requests.get(api_url, headers=headers, timeout=10)
                if api_response.status_code == 200:
                    api_data = api_response.json()
                    print(f"âœ… ä»APIè·å–æ•°æ®: {api_url}")
                    print(json.dumps(api_data, ensure_ascii=False, indent=2)[:500])
                    return parse_api_data(api_data)
            except Exception as e:
                continue
        
        # å¦‚æœAPIæ–¹æ³•å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®ä¾›æµ‹è¯•
        print("âš ï¸ æ— æ³•ä»APIè·å–æ•°æ®ï¼Œè¿”å›ç¤ºä¾‹æ ¼å¼")
        
        return {
            'success': False,
            'message': 'éœ€è¦è¿›ä¸€æ­¥åˆ†æé¡µé¢ç»“æ„æˆ–ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–',
            'url': url,
            'example_format': {
                'hour_1_amount': 1234567.89,
                'hour_24_amount': 98765432.10,
                'hour_24_people': 45678,
                'total_position': 12345678901.23,
                'panic_index': 0.0037,
                'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

def parse_api_data(api_data):
    """
    è§£æAPIè¿”å›çš„æ•°æ®
    """
    try:
        # æ ¹æ®å®é™…APIå“åº”æ ¼å¼è§£æ
        result = {
            'hour_1_amount': 0,
            'hour_24_amount': 0,
            'hour_24_people': 0,
            'total_position': 0,
            'panic_index': 0,
            'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'success': True,
            'raw_data': api_data
        }
        
        # TODO: æ ¹æ®å®é™…APIæ ¼å¼å¡«å……æ•°æ®
        # ä¾‹å¦‚:
        # result['hour_24_amount'] = api_data.get('blast24h', 0)
        # result['total_position'] = api_data.get('totalPosition', 0)
        
        return result
        
    except Exception as e:
        return {
            'success': False,
            'error': f'è§£æå¤±è´¥: {str(e)}'
        }

def scrape_with_selenium():
    """
    ä½¿ç”¨SeleniumåŠ¨æ€çˆ¬å–ï¼ˆå¦‚æœéœ€è¦ï¼‰
    """
    try:
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.chrome.options import Options
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.get('https://history.btc123.fans/baocang/')
        
        # ç­‰å¾…æ•°æ®åŠ è½½
        time.sleep(5)
        
        # æŸ¥æ‰¾åŒ…å«æ•°æ®çš„å…ƒç´ 
        # ä¾‹å¦‚: driver.find_element(By.CLASS_NAME, 'kuang')
        
        # æå–æ•°æ®
        page_text = driver.page_source
        
        driver.quit()
        
        # è§£ææ•°æ®
        soup = BeautifulSoup(page_text, 'html.parser')
        # ... è¿›ä¸€æ­¥è§£æ
        
        return {'success': True}
        
    except ImportError:
        print("âš ï¸ Seleniumæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–")
        return {'success': False, 'error': 'Selenium not installed'}
    except Exception as e:
        print(f"âŒ Seleniumçˆ¬å–å¤±è´¥: {str(e)}")
        return {'success': False, 'error': str(e)}

if __name__ == '__main__':
    print("="*70)
    print("ğŸ•·ï¸  çˆ†ä»“æ•°æ®çˆ¬è™«æµ‹è¯•")
    print("="*70)
    
    data = scrape_liquidation_data()
    
    print("\nğŸ“Š çˆ¬å–ç»“æœ:")
    print(json.dumps(data, ensure_ascii=False, indent=2))
    
    if data.get('success'):
        print("\nâœ… æ•°æ®è·å–æˆåŠŸ")
        if 'panic_index' in data:
            print(f"ææ…Œæ¸…æ´—æŒ‡æ•°: {data['panic_index']:.6f}")
    else:
        print("\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥å¼€å‘")
        print("ğŸ’¡ å»ºè®®:")
        print("  1. ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æŸ¥æ‰¾APIç«¯ç‚¹")
        print("  2. ä½¿ç”¨Selenium/Playwrightè¿›è¡ŒåŠ¨æ€çˆ¬å–")
        print("  3. è”ç³»ç½‘ç«™æä¾›æ–¹è·å–APIæ–‡æ¡£")
